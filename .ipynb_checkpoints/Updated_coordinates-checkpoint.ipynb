{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T11:14:47.053652Z",
     "start_time": "2020-02-21T11:14:47.050939Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import math \n",
    "import os\n",
    "\n",
    "from opencage.geocoder import OpenCageGeocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import xlsx file and store each sheet in to a df list\n",
    "xl_file = pd.ExcelFile('./data.xls',)\n",
    "\n",
    "dfs = {sheet_name: xl_file.parse(sheet_name) \n",
    "          for sheet_name in xl_file.sheet_names}\n",
    "\n",
    "# Data from each sheet can be accessed via key\n",
    "keyList = list(dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each sheet as csv to improve performance (these csv files will be used for app.py)\n",
    "for key, df in dfs.items():\n",
    "    dfs[key].to_csv('./raw_data/{}.csv'.format(key), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-21T11:14:52.459673Z",
     "start_time": "2020-02-21T11:14:48.641735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data cleansing\n",
    "for key, df in dfs.items():\n",
    "    dfs[key].loc[:,'Confirmed'].fillna(value=0, inplace=True)\n",
    "    dfs[key].loc[:,'Deaths'].fillna(value=0, inplace=True)\n",
    "    dfs[key].loc[:,'Recovered'].fillna(value=0, inplace=True)\n",
    "    dfs[key]=dfs[key].astype({'Confirmed':'int64', 'Deaths':'int64', 'Recovered':'int64'})\n",
    "    # Change as China for coordinate search\n",
    "    dfs[key]=dfs[key].replace({'Country/Region':'Mainland China'}, 'China')\n",
    "    # Add a zero to the date so can be convert by datetime.strptime as 0-padded date\n",
    "    dfs[key]['Last Update'] = '0' + dfs[key]['Last Update']\n",
    "    # Convert time as Australian eastern daylight time\n",
    "    dfs[key]['Date_last_updated_AEDT'] = [datetime.strptime(d, '%m/%d/%Y %H:%M') for d in dfs[key]['Last Update']]\n",
    "    dfs[key]['Date_last_updated_AEDT'] = dfs[key]['Date_last_updated_AEDT'] + timedelta(hours=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code of the following cell is for generating cumulative data for lineplot. They should run when updating heroku server folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative data of each region are generated!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./cumulative_data'):\n",
    "    os.makedirs('./cumulative_data')\n",
    "\n",
    "for Region in set(dfs[keyList[0]]['Country/Region']):\n",
    "    # Function for generating cumulative line plot for each Country/Region\n",
    "    CaseType = ['Confirmed', 'Recovered', 'Deaths']\n",
    "\n",
    "    # Construct confirmed cases dataframe for line plot\n",
    "    df_region = pd.DataFrame(columns=['Confirmed', 'Recovered', 'Deaths', 'Date_last_updated_AEDT'])\n",
    "\n",
    "    for key, df in dfs.items():\n",
    "        # As Country name will not be in the dataframe when there is no cases\n",
    "        if Region in list(df['Country/Region']):\n",
    "            dfTpm = df.groupby(['Country/Region']).agg({'Confirmed':np.sum, \n",
    "                                                        'Recovered':np.sum, \n",
    "                                                        'Deaths':np.sum, \n",
    "                                                        'Date_last_updated_AEDT':'first'})\n",
    "            df_region = df_region.append(dfTpm.loc[Region, :])        \n",
    "        else:\n",
    "            dfTpm2 = pd.DataFrame({'Confirmed':[0],\n",
    "                                   'Recovered':[0],\n",
    "                                   'Deaths':[0],\n",
    "                                   'Date_last_updated_AEDT':[df['Date_last_updated_AEDT'][0]]}, index=[Region])\n",
    "            df_region = df_region.append(dfTpm2)\n",
    "\n",
    "    # Select the latest data from a given date\n",
    "\n",
    "    #df_region = df_region.groupby(by=df_region['Date_last_updated_AEDT'], sort=False).first()\n",
    "    df_region['date_day']=[d.date() for d in df_region['Date_last_updated_AEDT']]\n",
    "    df_region=df_region.groupby(by=df_region['date_day'], sort=False).first()\n",
    "\n",
    "    df_region=df_region.reset_index(drop=True)\n",
    "    df_region.to_csv('./cumulative_data/{}.csv'.format(Region), index = False)\n",
    "\n",
    "print('Cumulative data of each region are generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part is for saving all coordinates as my own database. By doing so, `opencage.geocoder` does not need to go through all regions everytime (as most regions are already have coordinates in this database). Only new added regions will be called for coordinates via `opencage.geocoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinateCalling(queryData):\n",
    "    '''\n",
    "    Using opencage.geocoder to call coordinates for these regions\n",
    "    Add coordinates for each area in the list for the latest table sheet\n",
    "    As there are limit for free account, we only call coordinates for the latest table sheet\n",
    "    '''\n",
    "    key = 'b33700b33d0a446aa6e16c0b57fc82d1'  # get api key from:  https://opencagedata.com\n",
    "    geocoder = OpenCageGeocode(key)\n",
    "\n",
    "    list_lat = []   # create empty lists\n",
    "    list_long = []  \n",
    "\n",
    "    for index, row in queryData.iterrows(): # iterate over rows in dataframe\n",
    "\n",
    "        City = row['Province/State']\n",
    "        State = row['Country/Region']\n",
    "\n",
    "        # Note that 'nan' is float\n",
    "        if type(City) is str:\n",
    "            query = str(City)+','+str(State)\n",
    "            results = geocoder.geocode(query)   \n",
    "            lat = results[0]['geometry']['lat']\n",
    "            long = results[0]['geometry']['lng']\n",
    "\n",
    "            list_lat.append(lat)\n",
    "            list_long.append(long)\n",
    "        else:\n",
    "            query = str(State)\n",
    "            results = geocoder.geocode(query)   \n",
    "            lat = results[0]['geometry']['lat']\n",
    "            long = results[0]['geometry']['lng']\n",
    "\n",
    "            list_lat.append(lat)\n",
    "            list_long.append(long)\n",
    "\n",
    "    # create new columns from lists    \n",
    "    queryData['lat'] = list_lat   \n",
    "    queryData['lon'] = list_long\n",
    "    \n",
    "    return queryData\n",
    "    print('Coordinate data are generated!')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved\n"
     ]
    }
   ],
   "source": [
    "# Import coordinate database\n",
    "GeoDB = pd.read_csv('./coordinatesDB.csv')\n",
    "\n",
    "# Save the latest data into targetData\n",
    "targetData = dfs[keyList[0]]\n",
    "\n",
    "# Assign coordinates to regions from coordinates database\n",
    "resultData = pd.merge(targetData, GeoDB, how='left', on=['Province/State', 'Country/Region'])\n",
    "\n",
    "# Find regions do not have coordinates\n",
    "queryData = resultData.loc[resultData['lat'].isnull()]\n",
    "queryData = queryData[['Province/State', 'Country/Region']]\n",
    "\n",
    "\n",
    "if queryData.shape[0] != 0:\n",
    "    coordinateCalling(queryData)\n",
    "    # Add the new coordinates into coordinates database\n",
    "    catList = [GeoDB, queryData]\n",
    "    GeoDB = pd.concat(catList, ignore_index=True)\n",
    "    # Save the coordinates database\n",
    "    GeoDB.to_csv('./coordinatesDB.csv', index = False)\n",
    "    # Assign coordinates to all regions using the latest coordinates database\n",
    "    finalData = pd.merge(targetData, GeoDB, how='left', on=['Province/State', 'Country/Region'] )\n",
    "    \n",
    "    # To check if there is still regions without coordinates (There should not be)\n",
    "    if finalData.loc[finalData['lat'].isnull()].shape[0] == 0:\n",
    "        # Save the data for heroku app\n",
    "        finalData.to_csv('./{}_data.csv'.format(keyList[0]), index = False)\n",
    "    else:\n",
    "        print('Please check your data') \n",
    "else:\n",
    "    # Assign coordinates to all regions using the latest coordinates database\n",
    "    finalData = pd.merge(targetData, GeoDB, how='left', on=['Province/State', 'Country/Region'] )\n",
    "    # Save the data for heroku app\n",
    "    finalData.to_csv('./{}_data.csv'.format(keyList[0]), index = False)\n",
    "    print('Data has been saved')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all csv files and copy to heroku folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A variable for using in bash \n",
    "# Refer to https://stackoverflow.com/questions/19579546/can-i-access-python-variables-within-a-bash-or-script-ipython-notebook-c\n",
    "fileNmae = keyList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been transferred to heroku folder.\n",
      "You are now good to update heroku app!\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$fileNmae\"\n",
    "cp ./data.xls ../../heroku_app/dash_coronavirus_2019/\n",
    "cp ./raw_data/*.csv ../../heroku_app/dash_coronavirus_2019/raw_data/\n",
    "cp ./cumulative_data/*.csv ../../heroku_app/dash_coronavirus_2019/cumulative_data/\n",
    "cp ./$1_data.csv ../../heroku_app/dash_coronavirus_2019/\n",
    "echo \"All files have been transferred to heroku folder.\"\n",
    "echo \"You are now good to update heroku app!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
